# Hallucination in Large Foundation Models
This repository will be updated to include all the contemporary papers related to hallucination in foundation models. We broadly categorize the papers into **four** major categories as follows.

## Text
### LLMs
1. [SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/pdf/2303.08896.pdf)

### Multilingual LLMs
1. [Hallucinations in Large Multilingual Translation Models](https://arxiv.org/pdf/2303.16104.pdf)

### Domain-specific LLMs
1. [Med-HALT: Medical Domain Hallucination Test for Large Language Models](https://arxiv.org/pdf/2308.11764.pdf)
1. [ChatLawLLM](https://arxiv.org/pdf/2306.16092.pdf)

## Image

1. [Evaluating Object Hallucination in Large Vision-Language Models](https://arxiv.org/pdf/2305.10355.pdf)
1. [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/pdf/2308.06394.pdf)
1. [Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training](https://arxiv.org/pdf/2210.07688.pdf)
1. [Hallucination Improves the Performance of Unsupervised Visual Representation Learning](https://arxiv.org/pdf/2307.12168.pdf)


## Video



## Audio
